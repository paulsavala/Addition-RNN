{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning LSTMs\n",
    "The point of this notebook is just to mess with LSTMs more (in particular in the seq2seq setting) to get more comfortable with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an model using Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(num_features, num_timesteps, num_units=10):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=num_units, input_shape=(num_timesteps, num_features)))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model using the Functional API\n",
    "This is done roughly following [here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_func(num_timesteps, num_features, num_units=10, return_state=False, return_sequences=False):\n",
    "    lstm_input = Input(shape=(num_timesteps, num_features))\n",
    "    lstm = LSTM(units=num_units, return_state=return_state, return_sequences=return_sequences)\n",
    "    lstm_output = lstm(lstm_input)\n",
    "    model = Model(lstm_input, lstm_output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model, lstm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data for the model\n",
    "We'll one-hot the digits between 0 and 9 (inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict()\n",
    "\n",
    "for i in range(10):\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[i] = 1\n",
    "    mapping[i] = one_hot\n",
    "\n",
    "def reverse_mapping(one_hot):\n",
    "    i = int(np.argwhere(one_hot == 1))\n",
    "    return i\n",
    "\n",
    "# Take a (possibly) multidigit integer and one-hot it. Return a matrix of one-hot vectors representing each digit.\n",
    "def one_hot(s, mapping):\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    one_hot_s = np.zeros((len(s), 10))\n",
    "    for i, c in enumerate(s):\n",
    "        one_hot_s[i] = mapping[int(c)]\n",
    "    return one_hot_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use that to generate 100 training samples for the model. The input will be digits of length `digit_length`, and the output will be the sum of the digits mod 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(digit_length, n_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_samples):\n",
    "        x = str(np.random.randint(10**(digit_length-1), 10**digit_length))\n",
    "        y = sum(int(c) for c in x) % 10\n",
    "    return str(x), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "DIGIT_LENGTH = 3\n",
    "NUM_CHARS = 10\n",
    "\n",
    "X = np.zeros((NUM_SAMPLES, DIGIT_LENGTH, NUM_CHARS))\n",
    "Y = np.zeros((NUM_SAMPLES, NUM_CHARS))\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    x, y = generate_sample(DIGIT_LENGTH)\n",
    "    X[i] = one_hot(x, mapping)\n",
    "    Y[i] = one_hot(y, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the model\n",
    "So what exactly is returned from an LSTM? Not the entire model (which just returns a `model`), but the LSTM layer itself? Let's examine it step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func, lstm_output = my_model_func(num_timesteps=DIGIT_LENGTH, \\\n",
    "                                        num_features=NUM_CHARS, \\\n",
    "                                        return_state=False, \\\n",
    "                                        return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 10])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), an LSTM returns its final hidden state by default. That is, it returns the hidden state at the *last timestep only*. Since each hidden state is determined (in part) by the previous hidden states, this final hidden state can be thought of as encapsulating the information from all previous hidden state, cell states and inputs. \n",
    "\n",
    "Recall that the hidden state (i.e. $\\vec{h}=(h_1, h_2, \\ldots, h_{\\text{num_timesteps}})$) is actually a matrix, not a vector. Each $h_t$ is a vector itself of dimension `num_units`. So $\\vec{h}$ is of dimension `(num_timesteps, num_units)`. That is why the return shape of even just the final hidden state (i.e. $h_{\\text{num_timesteps}$) is still of dimension 10. Recall that the first entry (`None` in this case) refers to the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify `return_state=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func, lstm_output = my_model_func(num_timesteps=DIGIT_LENGTH, \\\n",
    "                                        num_features=NUM_CHARS, \\\n",
    "                                        return_state=True, \\\n",
    "                                        return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_16/Identity:0' shape=(None, 10) dtype=float32>,\n",
       " <tf.Tensor 'lstm_16/Identity_1:0' shape=(None, 10) dtype=float32>,\n",
       " <tf.Tensor 'lstm_16/Identity_2:0' shape=(None, 10) dtype=float32>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a list is returned. In fact, it consists of the triple (final hidden state, final hidden state (again), final cell state), as discussed [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). It might seem strange to return the final hidden state twice, but this is because while the LSTM output by default is the final hidden state, this can be modified. By passing `return_sequences=True`, the LSTM output will instead be the hidden state for *all* timesteps. Therefore, if you want to use the final hidden state, rather than needing to pull it from the LSTM output, you can just request it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func, lstm_output = my_model_func(num_timesteps=DIGIT_LENGTH, \\\n",
    "                                        num_features=NUM_CHARS, \\\n",
    "                                        return_state=True, \\\n",
    "                                        return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_17/Identity:0' shape=(None, 3, 10) dtype=float32>,\n",
       " <tf.Tensor 'lstm_17/Identity_1:0' shape=(None, 10) dtype=float32>,\n",
       " <tf.Tensor 'lstm_17/Identity_2:0' shape=(None, 10) dtype=float32>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
