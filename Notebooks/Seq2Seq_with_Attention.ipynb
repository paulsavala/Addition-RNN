{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tnxXKDjq3jEL",
    "outputId": "b3fd78fc-a567-4313-bfab-5a6a312b4aeb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE0wJLpPIF42"
   },
   "source": [
    "## Load integer addition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PCMJUn0qIH97"
   },
   "outputs": [],
   "source": [
    "def load(data_type, n_terms, n_digits):\n",
    "    term_dig_dir = Path(f'{n_terms}term_{n_digits}digs')\n",
    "\n",
    "    dir = Path('../Code') / Path('data') / Path(data_type) / term_dig_dir\n",
    "    X_train = np.load(dir / Path('X_train.npy'), allow_pickle=True)\n",
    "    X_test = np.load(dir / Path('X_test.npy'), allow_pickle=True)\n",
    "    y_train = np.load(dir / Path('y_train.npy'), allow_pickle=True)\n",
    "    y_test = np.load(dir / Path('y_test.npy'), allow_pickle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "srhN7ga8Ic3V"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load('random', 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4ddOWA0LSvA3",
    "outputId": "351b8758-e418-4921-d302-c453846f1e23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gi1V-VefSyEL",
    "outputId": "9c0a6057-14f4-4557-a2c2-26bac4602dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 10,  7,  1, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BAQJvHQn-Dmw",
    "outputId": "4bb23cc7-f104-4ca6-b8ac-7187a9a8b85c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zz0emBL7APlP",
    "outputId": "607f0494-f260-4fe9-ac25-64f4f7a03d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  1,  0,  2, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cg45pl4pIu_p",
    "outputId": "aed23dd0-396b-47c8-9b59-8fac030e70b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 6), (7000, 5))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Mjr0RK5jIyTS",
    "outputId": "977e75cb-95e0-4257-fa80-ee9528dda5c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 6), (3000, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qNRIjWaM1hal"
   },
   "outputs": [],
   "source": [
    "def char_to_int_map(max_value=9, min_value=0):\n",
    "    char_to_int = {str(n): n for n in range(min_value, max_value+1)}\n",
    "    n_terms = max_value - min_value + 1\n",
    "    char_to_int['+'] = n_terms\n",
    "    char_to_int['\\t'] = n_terms + 1\n",
    "    char_to_int['\\n'] = n_terms + 2\n",
    "    char_to_int[' '] = n_terms + 3\n",
    "    return char_to_int\n",
    "\n",
    "char_to_int = char_to_int_map()\n",
    "int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "\n",
    "def decode(v):\n",
    "    return int_to_char[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sovyIeQ3CbBI",
    "outputId": "b2301d88-8d53-47b7-e66c-046187379579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7+12+91\\n '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([decode(x) for x in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8rtyOakQCO6J",
    "outputId": "954f904e-5558-4d14-c5cc-052ae33b0afb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t110\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([decode(x) for x in y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = X_train.shape[0] // BATCH_SIZE\n",
    "embedding_dim = 13\n",
    "units = 256\n",
    "bidir = True\n",
    "vocab_inp_size = len(char_to_int)\n",
    "vocab_tar_size = len(char_to_int)\n",
    "\n",
    "# Data is already padded, so we can just grab any one\n",
    "max_length_inp = len(X_train[0])\n",
    "max_length_targ = len(y_train[0])\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "c242e480-3713-462c-8196-019dc46ca4cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 9]), TensorShape([32, 5]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_ds))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "\n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, bidir):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.bidir = bidir\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name='Encoder_Embedding')\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       name='Encoder_GRU')\n",
    "        if bidir:\n",
    "            self.bidir_gru = tf.keras.layers.Bidirectional(self.gru, name='Encoder_BidirGRU')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        if not self.bidir:\n",
    "            output, state = self.gru(x, initial_state = hidden)\n",
    "        else:\n",
    "            hidden = [hidden, hidden]\n",
    "            output, forward_state, reverse_state = self.bidir_gru(x, initial_state = hidden)\n",
    "            state = tf.concat([forward_state, reverse_state], axis=1)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, bidir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, bidir):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units, name='Attn_W1')\n",
    "        self.W2 = tf.keras.layers.Dense(units, name='Attn_W2')\n",
    "        self.V = tf.keras.layers.Dense(1, name='Attn_V')\n",
    "        self.bidir = bidir\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, bidir):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.bidir = bidir\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name='Decoder_Embedding')\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform',\n",
    "                                       name='Decoder_GRU')\n",
    "        if bidir:\n",
    "            self.bidir_gru = tf.keras.layers.Bidirectional(self.gru, name='Decoder_BidirGRU')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size, name='Decoder_FC')\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units, self.bidir)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        if not self.bidir:\n",
    "            output, state = self.gru(x)\n",
    "        else:\n",
    "            output, forward_state, reverse_state = self.bidir_gru(x)\n",
    "            state = tf.concat([forward_state, reverse_state], axis=1)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, bidir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # Mask whitespace first\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, char_to_int[' ']))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        # print(f'enc_output.shape = {enc_output.shape}')\n",
    "        # print(f'enc_hidden.shape = {enc_hidden.shape}')\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([char_to_int['\\t']] * BATCH_SIZE, 1)\n",
    "        dec_input = tf.dtypes.cast(dec_input, tf.float32)\n",
    "        # print(f'dec_input.shape = {dec_input.shape}')\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            # print(f'predictions.shape = {predictions.shape}')\n",
    "            # print(f'dec_hidden.shape = {dec_hidden.shape}')\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "ddefjBMa3jF0",
    "outputId": "1d76d8e3-66e3-4d2a-aff2-00751c9a688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.2315\n",
      "Time taken for 1 epoch 21.358434200286865 sec\n",
      "\n",
      "Epoch 2 Loss 0.9888\n",
      "Time taken for 1 epoch 13.86460280418396 sec\n",
      "\n",
      "Epoch 3 Loss 0.9604\n",
      "Time taken for 1 epoch 16.978954076766968 sec\n",
      "\n",
      "Epoch 4 Loss 0.9051\n",
      "Time taken for 1 epoch 13.825186967849731 sec\n",
      "\n",
      "Epoch 5 Loss 0.7358\n",
      "Time taken for 1 epoch 12.523950099945068 sec\n",
      "\n",
      "Epoch 6 Loss 0.5397\n",
      "Time taken for 1 epoch 12.253431797027588 sec\n",
      "\n",
      "Epoch 7 Loss 0.3576\n",
      "Time taken for 1 epoch 11.092602014541626 sec\n",
      "\n",
      "Epoch 8 Loss 0.2800\n",
      "Time taken for 1 epoch 13.771574020385742 sec\n",
      "\n",
      "Epoch 9 Loss 0.1879\n",
      "Time taken for 1 epoch 13.155783891677856 sec\n",
      "\n",
      "Epoch 10 Loss 0.1527\n",
      "Time taken for 1 epoch 12.979116678237915 sec\n",
      "\n",
      "Epoch 11 Loss 0.1067\n",
      "Time taken for 1 epoch 13.403764009475708 sec\n",
      "\n",
      "Epoch 12 Loss 0.1046\n",
      "Time taken for 1 epoch 12.581562995910645 sec\n",
      "\n",
      "Epoch 13 Loss 0.0749\n",
      "Time taken for 1 epoch 12.053032875061035 sec\n",
      "\n",
      "Epoch 14 Loss 0.0662\n",
      "Time taken for 1 epoch 12.693876028060913 sec\n",
      "\n",
      "Epoch 15 Loss 0.0518\n",
      "Time taken for 1 epoch 12.577167272567749 sec\n",
      "\n",
      "Epoch 16 Loss 0.0651\n",
      "Time taken for 1 epoch 10.994278907775879 sec\n",
      "\n",
      "Epoch 17 Loss 0.0437\n",
      "Time taken for 1 epoch 9.848409175872803 sec\n",
      "\n",
      "Epoch 18 Loss 0.0251\n",
      "Time taken for 1 epoch 10.050012111663818 sec\n",
      "\n",
      "Epoch 19 Loss 0.0263\n",
      "Time taken for 1 epoch 9.932040214538574 sec\n",
      "\n",
      "Epoch 20 Loss 0.0634\n",
      "Time taken for 1 epoch 11.464189767837524 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_ds.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #   print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "        #                                                batch,\n",
    "        #                                                batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                  total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    # Preprocess the input\n",
    "    inputs = [char_to_int[i] for i in sentence]\n",
    "    inputs = np.array(inputs).reshape(1, -1)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # Start with a blank result\n",
    "    result = ''\n",
    "\n",
    "    # Initial hidden state is zeros\n",
    "    hidden = tf.zeros((1, units))\n",
    "\n",
    "    # Encode the input\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    # Use the encoder's final hidden state as the dec's initial hidden state\n",
    "    dec_hidden = enc_hidden\n",
    "  \n",
    "    # Start by feeding the decoder the starting character \n",
    "    dec_input = tf.expand_dims([char_to_int['\\t']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        # Continually decode the current result, and store the attention weights for plotting\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += int_to_char[predicted_id]\n",
    "\n",
    "        if predicted_id == char_to_int['\\n']:\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='Reds')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    sentence = list(sentence)\n",
    "    # Appending a \\n doesn't jive with padding, need to fix this\n",
    "    sentence[-1] = r'\\n'\n",
    "    predicted_sentence = list(predicted_sentence)\n",
    "    predicted_sentence[-1] = r'\\n'\n",
    "\n",
    "    xticks = [''] + sentence\n",
    "    yticks = [''] + predicted_sentence\n",
    "    ax.set_xticklabels(xticks, fontdict=fontdict)\n",
    "    ax.set_yticklabels(yticks, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    # attention_plot = attention_plot[:len(result), :len(sentence)]\n",
    "    plot_attention(attention_plot, sentence, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x157045e80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "HF0Y_DQtHiEu"
   },
   "outputs": [],
   "source": [
    "def generate_example_plot(input=None, idx=None):\n",
    "    output = None\n",
    "    if idx is None and input is None:\n",
    "        idx = np.random.randint(X_test.shape[1])\n",
    "    if input is None:\n",
    "        input = X_test[idx]\n",
    "        output = y_test[idx]\n",
    "    if not isinstance(input, str):\n",
    "        test_input = ''.join([decode(x) for x in input])\n",
    "    else:\n",
    "        test_input = input\n",
    "    translate(test_input)\n",
    "\n",
    "    if output is not None:\n",
    "        print(f'Actual: {\"\".join([decode(x) for x in output]).strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "WrAM0FDomq3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 26+5+37\n",
      " \n",
      "Predicted translation: 32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-c1de69a79c6e>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(xticks, fontdict=fontdict)\n",
      "<ipython-input-56-c1de69a79c6e>:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(yticks, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 68\n"
     ]
    }
   ],
   "source": [
    "generate_example_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E62256nsboMS"
   },
   "source": [
    "## Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NPoGWEuIwzER"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual = 90, result = 91\n",
      "Actual = 71, result = 61\n",
      "Actual = 43, result = 42\n"
     ]
    }
   ],
   "source": [
    "correct_cnt = 0\n",
    "incorrect_attn_plots = dict()\n",
    "\n",
    "for x, y in zip(X_test[:100], y_test[:100]):\n",
    "    input = ''.join([decode(i) for i in x])\n",
    "    result, sentence, attention_plot = evaluate(input)\n",
    "    result = result.strip()\n",
    "    actual = ''.join([decode(i) for i in y]).strip()\n",
    "    if result == actual:\n",
    "        correct_cnt += 1\n",
    "    else:\n",
    "        incorrect_attn_plots[(input, result, actual)] = attention_plot\n",
    "        print(f'Actual = {actual}, result = {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nq0_WdG6fiH9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 82+8, result = 91, actual = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-c1de69a79c6e>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(xticks, fontdict=fontdict)\n",
      "<ipython-input-25-c1de69a79c6e>:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(yticks, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "input = 70+1, result = 61, actual = 71\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAH3CAYAAACb0QqwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPqklEQVR4nO3dX4xmd13H8e+33RiNRKu7W2wvlkJATIgp6JoQ0eoN0VRivUEvJIYobgT/xgujaIlKNMQYDUaRLEnFtAnW6kWLhUCImkgNylQKlsIFKVQjhc5ujYXGVcp+vdhds91s2+mnM+fMn9creZKZ8zw7v29+mWzec86ZZ3pmCgCAZ+eKtQcAANiLRBQAQEBEAQAERBQAQEBEAQAERBQAQEBEAQAERNTT6O7Pdfdc5nH32rPtd939pu7+bHef6e57u/t7154Jtkt339Ddd3X3f5z/P+X1a88EPHsi6ul9V1Vdc9HjO6pqquov1xxqv+vuH6uqt1fV71bVK6rqH6vq/d19bNXBDoDufnd3/+bacxwAz6uq+6vqF6vqv1eeBbZNd3+6u29ae46liKinMTObM/OFC4+qurGqHisRtdN+uarePTPvmplPzczPV9XDVfXGleeCbTEz75uZN8/MX1XV2bXngW10Z1WJKJ6su7uqfqqqbpsZPznukO7+mqr6zqr64CVPfbCqvnv5iQB4Fu6sqtd095P64vxl6xPdfUd3P97dD3b361aacduIqK17dVW9sKretfYg+9yRqrqyqr54yfEvVtW3LD8OAM/CR+rc2dVXXea5t9S5yLq+qm6vqlv2+m0aImrrfrqqPjozH197ENgu3f3m7v7yhUdV/XhVPemYm/qBrZqZs1X13rr8Jb1bZ+a2mflMVd1cVU9U1Q1LzrfdRNQWdPfVde4bwlmonXeqqr5aVc+/5Pjzq+oLy4+z772zql5+0eOuyxzbWGEuYO96qvuiPnHhg5l5oqo2q+rqpYbaCYfWHmCPeH1V/U9VvWflOfa9mfnf7r63zl0+veOip15dVX+9zlT718w8WlWPXvi8u79UVY+e/0kRIPGhqrqmu182M5+86PhXLnnd1B4/mSOinsH5G8rfUFV/MTNfXnueA+IPqurW7v7nqrqnqn6mqq6tc2dIYM/r7udV1YvPf3pFVR3r7pfXuYD9t9UGg20wM2e6+wN17mzUJ5/p9XvZni7AhXx/Vb2kXMpbzMzcXlW/VFW/UVX3VdX3VNWNM/PQimPBdjpeVR87//i6qvqt8x//9ppDwTY6EG910DOz9gwAwD7S3d9c5+5jvW5mPt/dU1WvPf/eaBde87mq+uOZ+f2VxnzORBQAsO26+++q6vaZ2be3YrgnCgDYCW+tqqvWHmInORMFABBwYzkAQEBEAQAERNQWdfeJtWc4aOz58uz58uz58uz58vbrnouorduX3wC7nD1fnj1fnj1fnj1f3r7ccxEFABBY/Lfzjhw5PNcdO7bomtth89TpOnrk8NpjHCj2fHl7es/Pnl17gsjm6dN19PDe3POHPn7/2iNEztTU11avPUbkBd/+bWuPENk8/Z919PA3rT1G5N5/feDUzBy93HOLv0/UdceO1caH/37pZYF9bs48vvYIB84bD7907REOnHfc9Z61Rzhwrnzh9U/5J8dczgMACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACMQR1d3XdPefd/dmd5/p7ge6+/u2czgAgN3qUPKPuvuqqrqnqj5cVT9UVZtV9aKqemTbJgMA2MWiiKqqX6mqh2fmJy469tltmAcAYE9IL+f9SFX9U3ff3t2PdPd93f1z3d3bOBsAwK6VRtSLqupNVfVgVf1AVb29qt5WVT97uRd394nu3ujujc1Tp8MlAQB2jzSirqiqf5mZX5uZj83Mn1XVH9VTRNTMnJyZ4zNz/OiRw+msAAC7RhpRD1fVA5cc+1RVHXtu4wAA7A1pRN1TVS+95Ni3VtVDz20cAIC9IY2oP6yqV3b3r3f3i7v7tVX1C1X1J9s3GgDA7hVF1Mx8tM79ht6PVtX9VfU7VXVzVb1j2yYDANjF0veJqpm5u6ru3sZZAAD2DH87DwAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAIiCgAgIKIAAAKH1h4AYDvMo59fe4QD508fuX/tEQ6cX73m+rVH4CLORAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEDgaSOquz/d3TctNQwAwF7xTGei7qwqEQUAcImtRNRruvtJr+vu6e4T3X1Hdz/e3Q929+t2bkwAgN3lmSLqI1V1tqpedZnn3lLnIuv6qrq9qm7p7mOX+yLng2ujuzc2T51+LvMCAOwKTxtRM3O2qt5bl7+kd+vM3DYzn6mqm6vqiaq64Sm+zsmZOT4zx48eOfxcZwYAWN1Wfjvvqe6L+sSFD2bmiararKqrt2kuAIBdbSsR9aGquqa7X3bJ8a9c8vls8esBAOx5zxg9M3Omqj5QfksPAOD/bfXMkbc6AAC4yFYj6m+q6hXdfe1ODgMAsFcc2sqLZubR7r6nqn64qt45M32Z11y3zbMBAOxaW4qo895aVVft0BwAAHvKliNqZv52JwcBANhLvCUBAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEBARAEABEQUAEDg0OIr/tep+ur7bll82QPtS4+tPcGBc8VNb1h7hANn7vuHtUc4cPqVP7j2CAfO2/59Y+0RDpzfu/YlT/mcM1EAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAAERBQAQEFEAAIFFIqq7T3T3RndvbD72+BJLAgDsqEUiamZOzszxmTl+9Bu+foklAQB2lMt5AAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAAABEQUAEBBRAACBQ4uv+I1H6sobf3LxZYH9zf8rwNKciQIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACCwSUd19ors3untj89TpJZYEANhRi0TUzJycmeMzc/zokcNLLAkAsKNczgMACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACCwSUd19ors3untj89TpJZYEANhRi0TUzJycmeMzc/zokcNLLAkAsKNczgMACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAICAiAIACIgoAIBAz8yyC3ZvVtVDiy66PY5U1am1hzhg7Pny7Pny7Pny7Pny9vKev2Bmjl7uicUjaq/q7o2ZOb72HAeJPV+ePV+ePV+ePV/eft1zl/MAAAIiCgAgIKK27uTaAxxA9nx59nx59nx59nx5+3LP3RMFABBwJgoAICCiAAACIgoAICCiAAACIgoAIPB/c+QQpRE3zVMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "input = 36+7, result = 42, actual = 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for k, attention_plot in incorrect_attn_plots.items():\n",
    "    sentence, result, actual = k\n",
    "    print(f'input = {sentence.strip()}, result = {result}, actual = {actual}')\n",
    "    plot_attention(attention_plot, sentence, result)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Vug5BLqeia-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 0+12\n",
      " \n",
      "Predicted translation: 12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-c1de69a79c6e>:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(xticks, fontdict=fontdict)\n",
      "<ipython-input-25-c1de69a79c6e>:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(yticks, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_example_plot(input='0+12\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFEwdC9Gk4YV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seq2Seq with Attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
