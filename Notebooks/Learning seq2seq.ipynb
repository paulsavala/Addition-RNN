{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning seq2seq\n",
    "The point of this notebook is to understand and implement a simple sequence-to-sequence (seq2seq) model. I'll generally be following [this tutorial](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) on Keras' site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "The general process is as follows:\n",
    "- Split the model into an encoder and a decoder. \n",
    "- Encoder:\n",
    "    - The encoder takes the input sequence as its input and outputs its internal state (both cell and hidden states, but only at the final timestep)\n",
    "- Decoder:\n",
    "    - The decoder uses as initial states the output states from the encoder\n",
    "    - It uses as input the *target* sequence and is trained to predict the next timestep in the target sequence (again)\n",
    "    \n",
    "Note then that the entire purpose of the encoder is to generate a vector space representation of the input in terms of the hidden and cell states of the encoder. That is, the encoder serves to compress the information in the input sequencer.\n",
    "\n",
    "The role of the decoder is to use this compression as information to decode a target sequence. Note that we *do not* use the input sequence in the decoder. This is because all information from the target sequence is (in theory) already held in the hidden states from the encoder. Thus the role of the decoder is to use that information to work directly with the target sequence. \n",
    "\n",
    "Let's now build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(input_seq_length, vocab_size, batch_size, num_units_enc, target_seq_length, num_units_dec=None):\n",
    "    if num_units_dec is None:\n",
    "        num_units_dec = num_units_enc\n",
    "        \n",
    "    encoder_input = Input(shape=(input_seq_length, vocab_size), batch_size=batch_size, name='Encoder_Input')\n",
    "    encoder_lstm = LSTM(units=num_units_enc, return_state=True, name='Encoder_LSTM')\n",
    "    encoder_lstm_output = encoder_lstm(encoder_input)\n",
    "    encoder_states = encoder_lstm_output[1:]\n",
    "        \n",
    "    decoder_input = Input(shape=(target_seq_length, vocab_size), batch_size=batch_size, name='Decoder_Input')\n",
    "    # todo: Why is `return_sequences` needed? It relates to the input size of the final dense layer.\n",
    "    decoder_lstm = LSTM(units=num_units_dec, return_sequences=True, name='Decoder_LSTM')\n",
    "    decoder_lstm_output = decoder_lstm(decoder_input, initial_state=encoder_states) \n",
    "    decoder_dense = Dense(vocab_size, activation='softmax', name='Decoder_Dense')\n",
    "    decoder_output = decoder_dense(decoder_lstm_output)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "We'll do the simple two-digit addition problem, since data is easy to generate from scratch. So the input sequence will be a string (one-hotted later) of digits and '+', and the output will be the sum (also as a string). We will include an end-of-string token for both as well. Like in the notebook we can either encode them as lists of integers and then let an embedding layer handle representing them as vectors, or one-hot everything. For simplicity (and to see what effects the embedding layer has later on) we'll one-hot for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {str(n): n for n in range(10)}\n",
    "char_to_int[' '] = 10\n",
    "char_to_int['+'] = 11\n",
    "char_to_int['\\n'] = 12\n",
    "\n",
    "int_to_char = {v: k for k, v in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n, dim):\n",
    "    # One-hots a positive integer n\n",
    "    one_hot_n = np.zeros(dim)\n",
    "    one_hot_n[n] = 1\n",
    "    return one_hot_n\n",
    "    \n",
    "def undo_one_hot(v):\n",
    "    return np.argmax(v)\n",
    "\n",
    "def one_hot_matrix(M, vocab_size):\n",
    "    n_samples, seq_length = M.shape\n",
    "    M_oh = np.array([one_hot(r, vocab_size) for r in np.array(M).flatten()]).reshape((n_samples, seq_length, vocab_size))\n",
    "    return np.squeeze(M_oh) # In case this is a target vector, we don't want to include an unnecessary axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_terms=2, digit_length=2, max_target_digits=3, int_encoder=None, reverse=False):\n",
    "    x = []\n",
    "    for _ in range(num_terms):\n",
    "        x.append(np.random.randint(10**digit_length))\n",
    "        \n",
    "    y = np.sum(x)\n",
    "    \n",
    "    x_str = '+'.join(str(n) for n in x)\n",
    "    y_str = str(y)\n",
    "    \n",
    "    # Pad x so that is always has the same length. It should be of length digit_length for each digit, plus num_terms - 1 \"plus\" signs\n",
    "    x_str = x_str.rjust(num_terms * digit_length + num_terms - 1)\n",
    "    y_str = y_str.rjust(max_target_digits) # todo: Fix to be a calculated value\n",
    "    \n",
    "    if reverse:\n",
    "        x_str = x_str[::-1]\n",
    "\n",
    "    x_str += '\\n'\n",
    "    y_str += '\\n'\n",
    "    \n",
    "    x_list = list(x_str)\n",
    "    y_list = list(y_str)\n",
    "    \n",
    "    if int_encoder is not None:\n",
    "        assert isinstance(int_encoder, dict), 'int_encoder must be a dictionary mapping characters to integers'\n",
    "        x_list = [int_encoder[c] for c in x_list]\n",
    "        y_list = [int_encoder[c] for c in y_list]\n",
    "        \n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n_samples, num_terms=2, digit_length=2, max_target_digits=3, int_encoder=None, one_hot=False, reverse=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_samples):\n",
    "        x_sample, y_sample = generate_sample(num_terms, digit_length, max_target_digits, int_encoder, reverse)\n",
    "        X.append(x_sample)\n",
    "        y.append(y_sample)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if one_hot:\n",
    "        X = one_hot_matrix(X, len(int_encoder))\n",
    "        y = one_hot_matrix(y, len(int_encoder))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['7', '3', '+', '6', '7', '\\n'], ['1', '4', '0', '\\n'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(append_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 6, 11, 3, 1, 12], [10, 6, 7, 12])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(append_token=True, int_encoder=char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh, y_oh = generate_samples(n_samples=10, append_token=True, int_encoder=char_to_int, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 6, 13), (10, 4, 13))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oh.shape, y_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the model\n",
    "Let's run some data through our model to make sure everything works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TERMS = 2\n",
    "DIGIT_LENGTH = 2\n",
    "SEQ_LENGTH = NUM_TERMS * DIGIT_LENGTH + NUM_TERMS\n",
    "\n",
    "model = my_model(input_seq_length=SEQ_LENGTH, vocab_size=len(char_to_int), batch_size=1, num_units_enc=1, target_seq_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_prediction(model, X, y):\n",
    "    X_singleton = X_oh[0].reshape(1, *X_oh[0].shape)\n",
    "    y_singleton = y_oh[0].reshape(1, *y_oh[0].shape)\n",
    "\n",
    "    output = model.predict([X_singleton, y_singleton], batch_size=1)\n",
    "    return output\n",
    "\n",
    "def sample_from_softmax(output):\n",
    "    squeezed_output = np.squeeze(output)\n",
    "    sampled_output = np.random.choice(np.arange(len(squeezed_output)), p=squeezed_output)\n",
    "    return sampled_output\n",
    "\n",
    "def predict_one(model, X, y):\n",
    "    model_output = example_prediction(model, X, y)\n",
    "    sampled_pred = sample_from_softmax(model_output)\n",
    "    # todo: Make this output nicer to get a better idea what's actually being predicted/used\n",
    "    print(f'X = {X}')\n",
    "    print(f'y = {y}')\n",
    "    print(f'Pred = {sampled_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07845037, 0.07061684, 0.07993645, 0.08047733, 0.07797949,\n",
       "        0.07469489, 0.07416169, 0.07028104, 0.06670289, 0.08318146,\n",
       "        0.08559087, 0.07626706, 0.08165962]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prediction(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[ 8  3 11  9  4 12]\n",
      " [ 1  8 11  1  6 12]\n",
      " [ 3  6 11  1  3 12]\n",
      " [ 9  9 11  5  4 12]\n",
      " [10  1 11  3  1 12]\n",
      " [ 1  7 11  8  4 12]\n",
      " [ 6  1 11  9  8 12]\n",
      " [ 4  9 11  3  3 12]\n",
      " [10  0 11  4  2 12]\n",
      " [10  4 11  9  4 12]]\n",
      "y = [[ 1  7  7 12]\n",
      " [10  3  4 12]\n",
      " [10  4  9 12]\n",
      " [ 1  5  3 12]\n",
      " [10  3  2 12]\n",
      " [ 1  0  1 12]\n",
      " [ 1  5  9 12]\n",
      " [10  8  2 12]\n",
      " [10  4  2 12]\n",
      " [10  9  8 12]]\n",
      "Pred = 9\n"
     ]
    }
   ],
   "source": [
    "predict_one(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh, y_oh = generate_samples(n_samples=10**4, append_token=True, int_encoder=char_to_int, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(input_seq_length=SEQ_LENGTH, vocab_size=len(char_to_int), batch_size=10, num_units_enc=1, target_seq_length=3)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/30\n",
      "7000/7000 [==============================] - 13s 2ms/sample - loss: 2.3529 - accuracy: 0.3184 - val_loss: 2.1386 - val_accuracy: 0.3333\n",
      "Epoch 2/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.9947 - accuracy: 0.3459 - val_loss: 1.8739 - val_accuracy: 0.3671\n",
      "Epoch 3/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.7964 - accuracy: 0.3645 - val_loss: 1.7337 - val_accuracy: 0.3693\n",
      "Epoch 4/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.6945 - accuracy: 0.3673 - val_loss: 1.6621 - val_accuracy: 0.3737\n",
      "Epoch 5/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.6395 - accuracy: 0.3944 - val_loss: 1.6206 - val_accuracy: 0.4010\n",
      "Epoch 6/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.6071 - accuracy: 0.4024 - val_loss: 1.5961 - val_accuracy: 0.4003\n",
      "Epoch 7/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5882 - accuracy: 0.4019 - val_loss: 1.5821 - val_accuracy: 0.3996\n",
      "Epoch 8/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5767 - accuracy: 0.4033 - val_loss: 1.5726 - val_accuracy: 0.4003\n",
      "Epoch 9/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5690 - accuracy: 0.4009 - val_loss: 1.5663 - val_accuracy: 0.4020\n",
      "Epoch 10/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5635 - accuracy: 0.4044 - val_loss: 1.5618 - val_accuracy: 0.3979\n",
      "Epoch 11/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5594 - accuracy: 0.4016 - val_loss: 1.5581 - val_accuracy: 0.4009\n",
      "Epoch 12/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5562 - accuracy: 0.4026 - val_loss: 1.5553 - val_accuracy: 0.4009\n",
      "Epoch 13/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5539 - accuracy: 0.4022 - val_loss: 1.5531 - val_accuracy: 0.4003\n",
      "Epoch 14/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5520 - accuracy: 0.4027 - val_loss: 1.5515 - val_accuracy: 0.3979\n",
      "Epoch 15/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5504 - accuracy: 0.4018 - val_loss: 1.5502 - val_accuracy: 0.4003\n",
      "Epoch 16/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5492 - accuracy: 0.4022 - val_loss: 1.5489 - val_accuracy: 0.3979\n",
      "Epoch 17/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5480 - accuracy: 0.4013 - val_loss: 1.5477 - val_accuracy: 0.4009\n",
      "Epoch 18/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5466 - accuracy: 0.4032 - val_loss: 1.5461 - val_accuracy: 0.3979\n",
      "Epoch 19/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5448 - accuracy: 0.4109 - val_loss: 1.5437 - val_accuracy: 0.4229\n",
      "Epoch 20/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5422 - accuracy: 0.4169 - val_loss: 1.5405 - val_accuracy: 0.4153\n",
      "Epoch 21/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5384 - accuracy: 0.4201 - val_loss: 1.5357 - val_accuracy: 0.4234\n",
      "Epoch 22/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5333 - accuracy: 0.4210 - val_loss: 1.5301 - val_accuracy: 0.4218\n",
      "Epoch 23/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5274 - accuracy: 0.4238 - val_loss: 1.5240 - val_accuracy: 0.4307\n",
      "Epoch 24/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5215 - accuracy: 0.4308 - val_loss: 1.5180 - val_accuracy: 0.4291\n",
      "Epoch 25/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5162 - accuracy: 0.4291 - val_loss: 1.5130 - val_accuracy: 0.4313\n",
      "Epoch 26/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5115 - accuracy: 0.4297 - val_loss: 1.5086 - val_accuracy: 0.4311\n",
      "Epoch 27/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5075 - accuracy: 0.4299 - val_loss: 1.5054 - val_accuracy: 0.4319\n",
      "Epoch 28/30\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 1.5042 - accuracy: 0.4302 - val_loss: 1.5020 - val_accuracy: 0.4317\n",
      "Epoch 29/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.5014 - accuracy: 0.4311 - val_loss: 1.4995 - val_accuracy: 0.4309\n",
      "Epoch 30/30\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 1.4989 - accuracy: 0.4304 - val_loss: 1.4971 - val_accuracy: 0.4326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x65ef41bd0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_target = y_oh[:,:-1,:]\n",
    "output_target = y_oh[:,1:,:]\n",
    "model.fit([X_oh, input_target], output_target, epochs=30, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So training for 30 epochs with a single unit in the LSTM gives ~43% validation accuracy. \n",
    "\n",
    "Let's do it again, but mirroring the parameters set by the [Addition RNN](https://keras.io/examples/addition_rnn/) script at Keras. They train using three digits with 5k samples with 10% validation for 200 epochs with a batch size of 128 and with 128 units in the LSTM and achieve 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TERMS = 2\n",
    "DIGIT_LENGTH = 3\n",
    "SEQ_LENGTH = NUM_TERMS * DIGIT_LENGTH + NUM_TERMS\n",
    "\n",
    "X_oh, y_oh = generate_samples(n_samples=5*10**3, \\\n",
    "                              digit_length=DIGIT_LENGTH, \\\n",
    "                              max_target_digits=4, \\\n",
    "                              append_token=True, \\\n",
    "                              int_encoder=char_to_int, \\\n",
    "                              one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(input_seq_length=SEQ_LENGTH, vocab_size=len(char_to_int), batch_size=128, num_units_enc=128, target_seq_length=4)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 7s 1ms/sample - loss: 2.2103 - accuracy: 0.2530\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 2s 470us/sample - loss: 1.7638 - accuracy: 0.3296\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 2s 496us/sample - loss: 1.7350 - accuracy: 0.3408\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 3s 502us/sample - loss: 1.7261 - accuracy: 0.3435\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 2s 497us/sample - loss: 1.7174 - accuracy: 0.3496\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 3s 506us/sample - loss: 1.7085 - accuracy: 0.3485\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 3s 518us/sample - loss: 1.7005 - accuracy: 0.3503\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 3s 508us/sample - loss: 1.6904 - accuracy: 0.3491\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 3s 509us/sample - loss: 1.6775 - accuracy: 0.3541\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 3s 509us/sample - loss: 1.6664 - accuracy: 0.3589\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 3s 514us/sample - loss: 1.6600 - accuracy: 0.3572\n",
      "Epoch 12/200\n",
      "5000/5000 [==============================] - 3s 517us/sample - loss: 1.6491 - accuracy: 0.3643\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 1.6439 - accuracy: 0.3632\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.6360 - accuracy: 0.3671\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.6312 - accuracy: 0.3700\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.6233 - accuracy: 0.3729\n",
      "Epoch 17/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.6176 - accuracy: 0.3790\n",
      "Epoch 18/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.6100 - accuracy: 0.3810\n",
      "Epoch 19/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.5876 - accuracy: 0.3884\n",
      "Epoch 20/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.5627 - accuracy: 0.3965\n",
      "Epoch 21/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 1.5306 - accuracy: 0.4075\n",
      "Epoch 22/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.5073 - accuracy: 0.4168\n",
      "Epoch 23/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.4668 - accuracy: 0.4378\n",
      "Epoch 24/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.4471 - accuracy: 0.4401\n",
      "Epoch 25/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.4140 - accuracy: 0.4611\n",
      "Epoch 26/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.3841 - accuracy: 0.4757\n",
      "Epoch 27/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.3703 - accuracy: 0.4785\n",
      "Epoch 28/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 1.3514 - accuracy: 0.4895\n",
      "Epoch 29/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.3265 - accuracy: 0.5049\n",
      "Epoch 30/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.3243 - accuracy: 0.4972\n",
      "Epoch 31/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.3019 - accuracy: 0.5148\n",
      "Epoch 32/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.3005 - accuracy: 0.5121\n",
      "Epoch 33/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.2855 - accuracy: 0.5230\n",
      "Epoch 34/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.2761 - accuracy: 0.5229\n",
      "Epoch 35/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.2575 - accuracy: 0.5333\n",
      "Epoch 36/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.2429 - accuracy: 0.5405\n",
      "Epoch 37/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 1.2495 - accuracy: 0.5339\n",
      "Epoch 38/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.2292 - accuracy: 0.5447\n",
      "Epoch 39/200\n",
      "5000/5000 [==============================] - 3s 519us/sample - loss: 1.2183 - accuracy: 0.5490\n",
      "Epoch 40/200\n",
      "5000/5000 [==============================] - 3s 521us/sample - loss: 1.2105 - accuracy: 0.5526\n",
      "Epoch 41/200\n",
      "5000/5000 [==============================] - 3s 519us/sample - loss: 1.2007 - accuracy: 0.5541\n",
      "Epoch 42/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.1990 - accuracy: 0.5544\n",
      "Epoch 43/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 1.1921 - accuracy: 0.5555\n",
      "Epoch 44/200\n",
      "5000/5000 [==============================] - 3s 519us/sample - loss: 1.1754 - accuracy: 0.5664\n",
      "Epoch 45/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 1.1799 - accuracy: 0.5590\n",
      "Epoch 46/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 1.1977 - accuracy: 0.5444\n",
      "Epoch 47/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 1.1596 - accuracy: 0.5699\n",
      "Epoch 48/200\n",
      "5000/5000 [==============================] - 3s 521us/sample - loss: 1.1510 - accuracy: 0.5706\n",
      "Epoch 49/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.1780 - accuracy: 0.5541\n",
      "Epoch 50/200\n",
      "5000/5000 [==============================] - 3s 521us/sample - loss: 1.1436 - accuracy: 0.5756\n",
      "Epoch 51/200\n",
      "5000/5000 [==============================] - 3s 521us/sample - loss: 1.1269 - accuracy: 0.5825\n",
      "Epoch 52/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.1154 - accuracy: 0.5865\n",
      "Epoch 53/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.1132 - accuracy: 0.5874\n",
      "Epoch 54/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.1037 - accuracy: 0.5898\n",
      "Epoch 55/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.1033 - accuracy: 0.5872\n",
      "Epoch 56/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.1262 - accuracy: 0.5724\n",
      "Epoch 57/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.0929 - accuracy: 0.5929\n",
      "Epoch 58/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.0806 - accuracy: 0.5953\n",
      "Epoch 59/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.1033 - accuracy: 0.5839\n",
      "Epoch 60/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0901 - accuracy: 0.5900\n",
      "Epoch 61/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0725 - accuracy: 0.6003\n",
      "Epoch 62/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.0671 - accuracy: 0.5992\n",
      "Epoch 63/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0501 - accuracy: 0.6089\n",
      "Epoch 64/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0414 - accuracy: 0.6106\n",
      "Epoch 65/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0584 - accuracy: 0.6039\n",
      "Epoch 66/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0483 - accuracy: 0.6067\n",
      "Epoch 67/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.0254 - accuracy: 0.6197\n",
      "Epoch 68/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0306 - accuracy: 0.6137\n",
      "Epoch 69/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0166 - accuracy: 0.6206\n",
      "Epoch 70/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.0181 - accuracy: 0.6176\n",
      "Epoch 71/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.0091 - accuracy: 0.6233\n",
      "Epoch 72/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.0002 - accuracy: 0.6250\n",
      "Epoch 73/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.0139 - accuracy: 0.6170\n",
      "Epoch 74/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.9964 - accuracy: 0.6289\n",
      "Epoch 75/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.9870 - accuracy: 0.6306\n",
      "Epoch 76/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.9902 - accuracy: 0.6267\n",
      "Epoch 77/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.9963 - accuracy: 0.6199\n",
      "Epoch 78/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.9782 - accuracy: 0.6320\n",
      "Epoch 79/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.9798 - accuracy: 0.6334\n",
      "Epoch 80/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.9596 - accuracy: 0.6453\n",
      "Epoch 81/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9584 - accuracy: 0.6452\n",
      "Epoch 82/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.9595 - accuracy: 0.6380\n",
      "Epoch 83/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9691 - accuracy: 0.6309\n",
      "Epoch 84/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.9597 - accuracy: 0.6403\n",
      "Epoch 85/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9454 - accuracy: 0.6434\n",
      "Epoch 86/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.9586 - accuracy: 0.6353\n",
      "Epoch 87/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.9432 - accuracy: 0.6446\n",
      "Epoch 88/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.9324 - accuracy: 0.6499\n",
      "Epoch 89/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9195 - accuracy: 0.6594\n",
      "Epoch 90/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9120 - accuracy: 0.6625\n",
      "Epoch 91/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.9277 - accuracy: 0.6514\n",
      "Epoch 92/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.9031 - accuracy: 0.6647\n",
      "Epoch 93/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9040 - accuracy: 0.6647\n",
      "Epoch 94/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.9169 - accuracy: 0.6550\n",
      "Epoch 95/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.9084 - accuracy: 0.6564\n",
      "Epoch 96/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.9118 - accuracy: 0.6561\n",
      "Epoch 97/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.9223 - accuracy: 0.6470\n",
      "Epoch 98/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.8898 - accuracy: 0.6673\n",
      "Epoch 99/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.9013 - accuracy: 0.6597\n",
      "Epoch 100/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.8707 - accuracy: 0.6820\n",
      "Epoch 101/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8779 - accuracy: 0.6733\n",
      "Epoch 102/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8792 - accuracy: 0.6697\n",
      "Epoch 103/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.8738 - accuracy: 0.6733\n",
      "Epoch 104/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.8823 - accuracy: 0.6680\n",
      "Epoch 105/200\n",
      "5000/5000 [==============================] - 3s 540us/sample - loss: 0.8840 - accuracy: 0.6647\n",
      "Epoch 106/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.8565 - accuracy: 0.6829\n",
      "Epoch 107/200\n",
      "5000/5000 [==============================] - 3s 533us/sample - loss: 0.8528 - accuracy: 0.6846\n",
      "Epoch 108/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.8580 - accuracy: 0.6804\n",
      "Epoch 109/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.8541 - accuracy: 0.6795\n",
      "Epoch 110/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8521 - accuracy: 0.6830\n",
      "Epoch 111/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.8821 - accuracy: 0.6683\n",
      "Epoch 112/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.8413 - accuracy: 0.6906\n",
      "Epoch 113/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8236 - accuracy: 0.6981\n",
      "Epoch 114/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8344 - accuracy: 0.6882\n",
      "Epoch 115/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8483 - accuracy: 0.6794\n",
      "Epoch 116/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8482 - accuracy: 0.6798\n",
      "Epoch 117/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8516 - accuracy: 0.6786\n",
      "Epoch 118/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.8217 - accuracy: 0.6955\n",
      "Epoch 119/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8126 - accuracy: 0.6988\n",
      "Epoch 120/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8331 - accuracy: 0.6860\n",
      "Epoch 121/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8086 - accuracy: 0.6998\n",
      "Epoch 122/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8213 - accuracy: 0.6920\n",
      "Epoch 123/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.8156 - accuracy: 0.6960\n",
      "Epoch 124/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.8178 - accuracy: 0.6951\n",
      "Epoch 125/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8012 - accuracy: 0.7028\n",
      "Epoch 126/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.7936 - accuracy: 0.7067\n",
      "Epoch 127/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.8255 - accuracy: 0.6855\n",
      "Epoch 128/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7950 - accuracy: 0.7031\n",
      "Epoch 129/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7928 - accuracy: 0.7045\n",
      "Epoch 130/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.7846 - accuracy: 0.7084\n",
      "Epoch 131/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7919 - accuracy: 0.7039\n",
      "Epoch 132/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7740 - accuracy: 0.7134\n",
      "Epoch 133/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.8074 - accuracy: 0.6975\n",
      "Epoch 134/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7723 - accuracy: 0.7140\n",
      "Epoch 135/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7998 - accuracy: 0.6988\n",
      "Epoch 136/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.7866 - accuracy: 0.7014\n",
      "Epoch 137/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7749 - accuracy: 0.7128\n",
      "Epoch 138/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8244 - accuracy: 0.6871\n",
      "Epoch 139/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8070 - accuracy: 0.6936\n",
      "Epoch 140/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7757 - accuracy: 0.7129\n",
      "Epoch 141/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7485 - accuracy: 0.7264\n",
      "Epoch 142/200\n",
      "5000/5000 [==============================] - 3s 534us/sample - loss: 0.7402 - accuracy: 0.7290\n",
      "Epoch 143/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.7507 - accuracy: 0.7222\n",
      "Epoch 144/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7530 - accuracy: 0.7196\n",
      "Epoch 145/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7484 - accuracy: 0.7235\n",
      "Epoch 146/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7500 - accuracy: 0.7196\n",
      "Epoch 147/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7354 - accuracy: 0.7300\n",
      "Epoch 148/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7408 - accuracy: 0.7271\n",
      "Epoch 149/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.7428 - accuracy: 0.7234\n",
      "Epoch 150/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.7494 - accuracy: 0.7207\n",
      "Epoch 151/200\n",
      "5000/5000 [==============================] - 3s 532us/sample - loss: 0.7745 - accuracy: 0.7064\n",
      "Epoch 152/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7288 - accuracy: 0.7307\n",
      "Epoch 153/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.7141 - accuracy: 0.7409\n",
      "Epoch 154/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.8254 - accuracy: 0.6871\n",
      "Epoch 155/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7677 - accuracy: 0.7079\n",
      "Epoch 156/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7610 - accuracy: 0.7099\n",
      "Epoch 157/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7167 - accuracy: 0.7345\n",
      "Epoch 158/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6973 - accuracy: 0.7518\n",
      "Epoch 159/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7087 - accuracy: 0.7423\n",
      "Epoch 160/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7126 - accuracy: 0.7353\n",
      "Epoch 161/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7086 - accuracy: 0.7363\n",
      "Epoch 162/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6976 - accuracy: 0.7441\n",
      "Epoch 163/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7043 - accuracy: 0.7411\n",
      "Epoch 164/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.7140 - accuracy: 0.7366\n",
      "Epoch 165/200\n",
      "5000/5000 [==============================] - 3s 533us/sample - loss: 0.7012 - accuracy: 0.7408\n",
      "Epoch 166/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7127 - accuracy: 0.7347\n",
      "Epoch 167/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.7029 - accuracy: 0.7422\n",
      "Epoch 168/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6914 - accuracy: 0.7471\n",
      "Epoch 169/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6992 - accuracy: 0.7434\n",
      "Epoch 170/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.7338 - accuracy: 0.7238\n",
      "Epoch 171/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6918 - accuracy: 0.7456 - loss:\n",
      "Epoch 172/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6833 - accuracy: 0.7502\n",
      "Epoch 173/200\n",
      "5000/5000 [==============================] - 3s 540us/sample - loss: 0.6988 - accuracy: 0.7397\n",
      "Epoch 174/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6760 - accuracy: 0.7545\n",
      "Epoch 175/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6541 - accuracy: 0.7674\n",
      "Epoch 176/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.6558 - accuracy: 0.7634\n",
      "Epoch 177/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.6641 - accuracy: 0.7559\n",
      "Epoch 178/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6830 - accuracy: 0.7465\n",
      "Epoch 179/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6894 - accuracy: 0.7436\n",
      "Epoch 180/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6780 - accuracy: 0.7495\n",
      "Epoch 181/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.7038 - accuracy: 0.7366\n",
      "Epoch 182/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6490 - accuracy: 0.7665\n",
      "Epoch 183/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.6720 - accuracy: 0.7525\n",
      "Epoch 184/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.6903 - accuracy: 0.7451\n",
      "Epoch 185/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6539 - accuracy: 0.7599\n",
      "Epoch 186/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6617 - accuracy: 0.7581\n",
      "Epoch 187/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6499 - accuracy: 0.7625\n",
      "Epoch 188/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6456 - accuracy: 0.7647\n",
      "Epoch 189/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6495 - accuracy: 0.7622\n",
      "Epoch 190/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6207 - accuracy: 0.7789\n",
      "Epoch 191/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6385 - accuracy: 0.7681\n",
      "Epoch 192/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6627 - accuracy: 0.7549\n",
      "Epoch 193/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6615 - accuracy: 0.7592\n",
      "Epoch 194/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6246 - accuracy: 0.7754\n",
      "Epoch 195/200\n",
      "5000/5000 [==============================] - 3s 533us/sample - loss: 0.6406 - accuracy: 0.7641\n",
      "Epoch 196/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6413 - accuracy: 0.7638\n",
      "Epoch 197/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.6138 - accuracy: 0.7763\n",
      "Epoch 198/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6598 - accuracy: 0.7534\n",
      "Epoch 199/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.6234 - accuracy: 0.7740\n",
      "Epoch 200/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.6421 - accuracy: 0.7623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x66d602250>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_target = y_oh[:,:-1,:]\n",
    "output_target = y_oh[:,1:,:]\n",
    "model.fit([X_oh, input_target], output_target, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems promising. Let's generate a test set and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_samples(n_samples=1000, \\\n",
    "                              digit_length=DIGIT_LENGTH, \\\n",
    "                              max_target_digits=4, \\\n",
    "                              append_token=True, \\\n",
    "                              int_encoder=char_to_int, \\\n",
    "                              one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate([X_test, y_test[:, :-1, :]], y_test[:, 1:, :], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.9939\n",
      "accuracy = 0.6507\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(f'{n} = {v:.4f}' for n, v in list(zip(model.metrics_names, test_metrics))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So an improvement, but still well short of the 99% they claim.\n",
    "\n",
    "Next, let's try adding in \"reversing\". They claim this gives a good boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TERMS = 2\n",
    "DIGIT_LENGTH = 3\n",
    "SEQ_LENGTH = NUM_TERMS * DIGIT_LENGTH + NUM_TERMS\n",
    "\n",
    "X_oh, y_oh = generate_samples(n_samples=5*10**3, \\\n",
    "                              digit_length=DIGIT_LENGTH, \\\n",
    "                              max_target_digits=4, \\\n",
    "                              int_encoder=char_to_int, \\\n",
    "                              one_hot=True,\n",
    "                              reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(input_seq_length=SEQ_LENGTH, vocab_size=len(char_to_int), batch_size=128, num_units_enc=128, target_seq_length=4)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 7s 1ms/sample - loss: 2.1874 - accuracy: 0.2650\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 2s 464us/sample - loss: 1.7598 - accuracy: 0.3256\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 2s 492us/sample - loss: 1.7328 - accuracy: 0.3413\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 3s 503us/sample - loss: 1.7245 - accuracy: 0.3446\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 2s 494us/sample - loss: 1.7174 - accuracy: 0.3470\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 2s 499us/sample - loss: 1.7077 - accuracy: 0.3492\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 3s 504us/sample - loss: 1.6982 - accuracy: 0.3503\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 3s 516us/sample - loss: 1.6905 - accuracy: 0.3492\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 3s 517us/sample - loss: 1.6831 - accuracy: 0.3533\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 3s 506us/sample - loss: 1.6748 - accuracy: 0.3512\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 3s 508us/sample - loss: 1.6655 - accuracy: 0.3573\n",
      "Epoch 12/200\n",
      "5000/5000 [==============================] - 3s 512us/sample - loss: 1.6559 - accuracy: 0.3620\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 3s 512us/sample - loss: 1.6502 - accuracy: 0.3614\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 3s 517us/sample - loss: 1.6466 - accuracy: 0.3668\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 3s 519us/sample - loss: 1.6417 - accuracy: 0.3623\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.6318 - accuracy: 0.3713\n",
      "Epoch 17/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.6241 - accuracy: 0.3731\n",
      "Epoch 18/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 1.6126 - accuracy: 0.3776\n",
      "Epoch 19/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.5948 - accuracy: 0.3859\n",
      "Epoch 20/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.5748 - accuracy: 0.3961\n",
      "Epoch 21/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.5582 - accuracy: 0.4085\n",
      "Epoch 22/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 1.5414 - accuracy: 0.4123\n",
      "Epoch 23/200\n",
      "5000/5000 [==============================] - 3s 553us/sample - loss: 1.5268 - accuracy: 0.4211\n",
      "Epoch 24/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.5154 - accuracy: 0.4235\n",
      "Epoch 25/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 1.4960 - accuracy: 0.4369\n",
      "Epoch 26/200\n",
      "5000/5000 [==============================] - 4s 710us/sample - loss: 1.4735 - accuracy: 0.4471\n",
      "Epoch 27/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.4526 - accuracy: 0.4536\n",
      "Epoch 28/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 1.4247 - accuracy: 0.4733\n",
      "Epoch 29/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.4033 - accuracy: 0.4723\n",
      "Epoch 30/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 1.3582 - accuracy: 0.4931\n",
      "Epoch 31/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 1.3155 - accuracy: 0.5012\n",
      "Epoch 32/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 1.2603 - accuracy: 0.5207\n",
      "Epoch 33/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 1.1963 - accuracy: 0.5434\n",
      "Epoch 34/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.1262 - accuracy: 0.5744\n",
      "Epoch 35/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 1.0559 - accuracy: 0.6008\n",
      "Epoch 36/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.9706 - accuracy: 0.6484\n",
      "Epoch 37/200\n",
      "5000/5000 [==============================] - 3s 519us/sample - loss: 0.9084 - accuracy: 0.6867\n",
      "Epoch 38/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 0.8439 - accuracy: 0.7090\n",
      "Epoch 39/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.8036 - accuracy: 0.7227\n",
      "Epoch 40/200\n",
      "5000/5000 [==============================] - 3s 522us/sample - loss: 0.7526 - accuracy: 0.7530\n",
      "Epoch 41/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 0.7045 - accuracy: 0.7779\n",
      "Epoch 42/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.6740 - accuracy: 0.7889\n",
      "Epoch 43/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.6428 - accuracy: 0.7974\n",
      "Epoch 44/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.6146 - accuracy: 0.8118\n",
      "Epoch 45/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.5718 - accuracy: 0.8378\n",
      "Epoch 46/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.5449 - accuracy: 0.8522\n",
      "Epoch 47/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.5157 - accuracy: 0.8654\n",
      "Epoch 48/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.4971 - accuracy: 0.8709\n",
      "Epoch 49/200\n",
      "5000/5000 [==============================] - 3s 520us/sample - loss: 0.4772 - accuracy: 0.8759\n",
      "Epoch 50/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.4468 - accuracy: 0.8937\n",
      "Epoch 51/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.4343 - accuracy: 0.8903\n",
      "Epoch 52/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.4148 - accuracy: 0.8954\n",
      "Epoch 53/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.3938 - accuracy: 0.9067\n",
      "Epoch 54/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.3792 - accuracy: 0.9103\n",
      "Epoch 55/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.3674 - accuracy: 0.9132\n",
      "Epoch 56/200\n",
      "5000/5000 [==============================] - 3s 523us/sample - loss: 0.3551 - accuracy: 0.9130\n",
      "Epoch 57/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.3344 - accuracy: 0.9250\n",
      "Epoch 58/200\n",
      "5000/5000 [==============================] - 3s 524us/sample - loss: 0.3200 - accuracy: 0.9286\n",
      "Epoch 59/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.3089 - accuracy: 0.9277\n",
      "Epoch 60/200\n",
      "5000/5000 [==============================] - 3s 535us/sample - loss: 0.2891 - accuracy: 0.9373\n",
      "Epoch 61/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.3166 - accuracy: 0.9197\n",
      "Epoch 62/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.2790 - accuracy: 0.9374\n",
      "Epoch 63/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.2681 - accuracy: 0.9381\n",
      "Epoch 64/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.2483 - accuracy: 0.9469\n",
      "Epoch 65/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.2424 - accuracy: 0.9470\n",
      "Epoch 66/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.2300 - accuracy: 0.9518\n",
      "Epoch 67/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.2200 - accuracy: 0.9560\n",
      "Epoch 68/200\n",
      "5000/5000 [==============================] - 3s 538us/sample - loss: 0.2938 - accuracy: 0.9072\n",
      "Epoch 69/200\n",
      "5000/5000 [==============================] - 3s 534us/sample - loss: 0.2312 - accuracy: 0.9428\n",
      "Epoch 70/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1995 - accuracy: 0.9596\n",
      "Epoch 71/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.1927 - accuracy: 0.9632\n",
      "Epoch 72/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.1780 - accuracy: 0.9688\n",
      "Epoch 73/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1793 - accuracy: 0.9646\n",
      "Epoch 74/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1697 - accuracy: 0.9686\n",
      "Epoch 75/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1682 - accuracy: 0.9679\n",
      "Epoch 76/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1624 - accuracy: 0.9704\n",
      "Epoch 77/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1568 - accuracy: 0.9708\n",
      "Epoch 78/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.1472 - accuracy: 0.9737\n",
      "Epoch 79/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1401 - accuracy: 0.9765\n",
      "Epoch 80/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.1422 - accuracy: 0.9730\n",
      "Epoch 81/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.1324 - accuracy: 0.9761\n",
      "Epoch 82/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.1292 - accuracy: 0.9779\n",
      "Epoch 83/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1198 - accuracy: 0.9821\n",
      "Epoch 84/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.1180 - accuracy: 0.9816\n",
      "Epoch 85/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.1130 - accuracy: 0.9839\n",
      "Epoch 86/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1095 - accuracy: 0.9839\n",
      "Epoch 87/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.1048 - accuracy: 0.9844\n",
      "Epoch 88/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.1006 - accuracy: 0.9862\n",
      "Epoch 89/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0985 - accuracy: 0.9869\n",
      "Epoch 90/200\n",
      "5000/5000 [==============================] - 3s 525us/sample - loss: 0.0974 - accuracy: 0.9862\n",
      "Epoch 91/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0978 - accuracy: 0.9845\n",
      "Epoch 92/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1019 - accuracy: 0.9821\n",
      "Epoch 93/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0989 - accuracy: 0.9833\n",
      "Epoch 94/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0895 - accuracy: 0.9863\n",
      "Epoch 95/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0837 - accuracy: 0.9880\n",
      "Epoch 96/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0828 - accuracy: 0.9878\n",
      "Epoch 97/200\n",
      "5000/5000 [==============================] - 3s 534us/sample - loss: 0.0845 - accuracy: 0.9865\n",
      "Epoch 98/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0754 - accuracy: 0.9912\n",
      "Epoch 99/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.1015 - accuracy: 0.9788\n",
      "Epoch 100/200\n",
      "5000/5000 [==============================] - 3s 535us/sample - loss: 0.0793 - accuracy: 0.9876\n",
      "Epoch 101/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0732 - accuracy: 0.9890\n",
      "Epoch 102/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0651 - accuracy: 0.9927\n",
      "Epoch 103/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0616 - accuracy: 0.9930\n",
      "Epoch 104/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0610 - accuracy: 0.9933\n",
      "Epoch 105/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0551 - accuracy: 0.9947\n",
      "Epoch 106/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0549 - accuracy: 0.9945\n",
      "Epoch 107/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0522 - accuracy: 0.9952\n",
      "Epoch 108/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0496 - accuracy: 0.9962\n",
      "Epoch 109/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0481 - accuracy: 0.9959\n",
      "Epoch 110/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0522 - accuracy: 0.9935\n",
      "Epoch 111/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1401 - accuracy: 0.9551\n",
      "Epoch 112/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0750 - accuracy: 0.9830\n",
      "Epoch 113/200\n",
      "5000/5000 [==============================] - 3s 541us/sample - loss: 0.0618 - accuracy: 0.9893\n",
      "Epoch 114/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0445 - accuracy: 0.9962\n",
      "Epoch 115/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0419 - accuracy: 0.9966\n",
      "Epoch 116/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0395 - accuracy: 0.9977\n",
      "Epoch 117/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0372 - accuracy: 0.9974\n",
      "Epoch 118/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0350 - accuracy: 0.9977\n",
      "Epoch 119/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0353 - accuracy: 0.9977\n",
      "Epoch 120/200\n",
      "5000/5000 [==============================] - 3s 533us/sample - loss: 0.0351 - accuracy: 0.9974\n",
      "Epoch 121/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0355 - accuracy: 0.9976\n",
      "Epoch 122/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0327 - accuracy: 0.9980\n",
      "Epoch 123/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0312 - accuracy: 0.9979\n",
      "Epoch 124/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0319 - accuracy: 0.9982\n",
      "Epoch 125/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.0288 - accuracy: 0.9982\n",
      "Epoch 126/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0297 - accuracy: 0.9980\n",
      "Epoch 127/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0287 - accuracy: 0.9984\n",
      "Epoch 128/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0269 - accuracy: 0.9987\n",
      "Epoch 129/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0266 - accuracy: 0.9984\n",
      "Epoch 130/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0311 - accuracy: 0.9970\n",
      "Epoch 131/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0423 - accuracy: 0.9928\n",
      "Epoch 132/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0894 - accuracy: 0.9736\n",
      "Epoch 133/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0697 - accuracy: 0.9803\n",
      "Epoch 134/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1033 - accuracy: 0.9685\n",
      "Epoch 135/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0846 - accuracy: 0.9743\n",
      "Epoch 136/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.0365 - accuracy: 0.9947\n",
      "Epoch 137/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0271 - accuracy: 0.9979\n",
      "Epoch 138/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0224 - accuracy: 0.9991\n",
      "Epoch 139/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0219 - accuracy: 0.9991\n",
      "Epoch 140/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.0198 - accuracy: 0.9992\n",
      "Epoch 141/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0189 - accuracy: 0.9992\n",
      "Epoch 142/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0184 - accuracy: 0.9993\n",
      "Epoch 143/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0180 - accuracy: 0.9996\n",
      "Epoch 144/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.0183 - accuracy: 0.9992\n",
      "Epoch 145/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0169 - accuracy: 0.9997\n",
      "Epoch 146/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0162 - accuracy: 0.9997\n",
      "Epoch 147/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0158 - accuracy: 0.9997\n",
      "Epoch 148/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0155 - accuracy: 0.9998\n",
      "Epoch 149/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0149 - accuracy: 0.9997\n",
      "Epoch 150/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0148 - accuracy: 0.9997\n",
      "Epoch 151/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0141 - accuracy: 0.9998\n",
      "Epoch 152/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0142 - accuracy: 0.9995\n",
      "Epoch 153/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0137 - accuracy: 0.9995\n",
      "Epoch 154/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0131 - accuracy: 0.9998\n",
      "Epoch 155/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0128 - accuracy: 0.9998\n",
      "Epoch 156/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0126 - accuracy: 0.9998\n",
      "Epoch 157/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0120 - accuracy: 0.9998\n",
      "Epoch 158/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0124 - accuracy: 0.9998\n",
      "Epoch 159/200\n",
      "5000/5000 [==============================] - 3s 536us/sample - loss: 0.0114 - accuracy: 0.9999\n",
      "Epoch 160/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0113 - accuracy: 0.9999\n",
      "Epoch 161/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0107 - accuracy: 0.9999\n",
      "Epoch 162/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0105 - accuracy: 0.9999\n",
      "Epoch 163/200\n",
      "5000/5000 [==============================] - 3s 531us/sample - loss: 0.0103 - accuracy: 0.9999\n",
      "Epoch 164/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0105 - accuracy: 0.9998\n",
      "Epoch 165/200\n",
      "5000/5000 [==============================] - 3s 534us/sample - loss: 0.0098 - accuracy: 0.9998\n",
      "Epoch 166/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0100 - accuracy: 0.9999\n",
      "Epoch 167/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0094 - accuracy: 0.9999\n",
      "Epoch 168/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0089 - accuracy: 0.9999\n",
      "Epoch 169/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0099 - accuracy: 0.9998\n",
      "Epoch 170/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0089 - accuracy: 0.9998\n",
      "Epoch 171/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0179 - accuracy: 0.9978\n",
      "Epoch 172/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.1403 - accuracy: 0.9549\n",
      "Epoch 173/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.2838 - accuracy: 0.9086\n",
      "Epoch 174/200\n",
      "5000/5000 [==============================] - 3s 535us/sample - loss: 0.0590 - accuracy: 0.9855\n",
      "Epoch 175/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0251 - accuracy: 0.9969\n",
      "Epoch 176/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0164 - accuracy: 0.9988\n",
      "Epoch 177/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0132 - accuracy: 0.9996\n",
      "Epoch 178/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0116 - accuracy: 0.9998\n",
      "Epoch 179/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0106 - accuracy: 0.9998\n",
      "Epoch 180/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0104 - accuracy: 0.9998\n",
      "Epoch 181/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0094 - accuracy: 0.9999\n",
      "Epoch 182/200\n",
      "5000/5000 [==============================] - 3s 533us/sample - loss: 0.0089 - accuracy: 0.9999\n",
      "Epoch 183/200\n",
      "5000/5000 [==============================] - 3s 538us/sample - loss: 0.0085 - accuracy: 0.9999\n",
      "Epoch 184/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0083 - accuracy: 0.9999\n",
      "Epoch 185/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0081 - accuracy: 0.9999\n",
      "Epoch 186/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0076 - accuracy: 0.9999\n",
      "Epoch 188/200\n",
      "5000/5000 [==============================] - 3s 527us/sample - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5000/5000 [==============================] - 3s 537us/sample - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0063 - accuracy: 0.9999\n",
      "Epoch 194/200\n",
      "5000/5000 [==============================] - 3s 532us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5000/5000 [==============================] - 3s 530us/sample - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5000/5000 [==============================] - 3s 526us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0056 - accuracy: 0.9999\n",
      "Epoch 199/200\n",
      "5000/5000 [==============================] - 3s 528us/sample - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5000/5000 [==============================] - 3s 529us/sample - loss: 0.0053 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x66ae019d0>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_target = y_oh[:,:-1,:]\n",
    "output_target = y_oh[:,1:,:]\n",
    "model.fit([X_oh, input_target], output_target, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_samples(n_samples=1000, \\\n",
    "                              digit_length=DIGIT_LENGTH, \\\n",
    "                              max_target_digits=4, \\\n",
    "                              int_encoder=char_to_int, \\\n",
    "                              one_hot=True, \\\n",
    "                              reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate([X_test, y_test[:, :-1, :]], y_test[:, 1:, :], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.1018\n",
      "accuracy = 0.9737\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(f'{n} = {v:.4f}' for n, v in list(zip(model.metrics_names, test_metrics))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. So reversing the input increased our test accuracy from 65% to 97%, and resulted in a nearly 90% reduction in the loss. That's incredible. Reverse reverse reverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
