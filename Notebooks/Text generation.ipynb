{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "The point of this notebook is to develop a (toy) application of LSTMs. We'll follow the [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation) to generate Shakespeare text. However, I'll be writing it from scratch, and only referencing the tutorial when I get stuck, or to check if I'm heading in the right direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/psavala/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 characters (65 unique characters)\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "print(f'{len(text)} characters ({len(unique_chars)} unique characters)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique characters: {unique_chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHARS = len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {c: i for i, c in enumerate(unique_chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "We'll create training data by selecting a number of characters to use for the input, say `seq_len`, then the `seq_len+1` character will be the target. The number of characters in these pairs will influence how much the LSTM can learn from the past. So we'll experiment with the results for different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(text, char_to_int_mapping, seq_length=5, n_samples=10**6):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, n_samples * seq_length, seq_length):\n",
    "        x = text[i:i+seq_length]\n",
    "        target = text[i+seq_length]\n",
    "        \n",
    "        x = np.array([char_to_int_mapping[c] for c in x])\n",
    "        target = char_to_int_mapping[target]\n",
    "        X.append(x)\n",
    "        y.append(target)\n",
    "        \n",
    "    X = np.array(X).reshape(n_samples, seq_length)\n",
    "    y = np.array(y).reshape(n_samples, 1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 5\n",
    "\n",
    "X, y = generate_samples(text, char_to_int, seq_length=SEQ_LENGTH, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "[[18 47 56 57 58]\n",
      " [ 1 15 47 58 47]\n",
      " [64 43 52 10  0]\n",
      " [14 43 44 53 56]\n",
      " [43  1 61 43  1]\n",
      " [54 56 53 41 43]\n",
      " [43 42  1 39 52]\n",
      " [63  1 44 59 56]\n",
      " [58 46 43 56  6]\n",
      " [ 1 46 43 39 56]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [64]\n",
      " [14]\n",
      " [43]\n",
      " [54]\n",
      " [43]\n",
      " [63]\n",
      " [58]\n",
      " [ 1]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious next step would be to one-hot our data. We do this below. However, it's also possible to replace one-hotting with a Keras `Embedding` layer. That is, rather than one-hotting we will ask it to learn representations on its own. We'll do that as well and compare the results. One potential issue (maybe?) is that while the model will learn representations for the characters through the embedding layer, the target values (which are also characters, and thus have a corresponding vector in the embedding layer) cannot see this embedding, so they have to be predicted as raw characters. It would be nice to convert them to their corresponding vectors and have *those* be predicted. Maybe it doesn't matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n, dim):\n",
    "    # One-hots a positive integer n\n",
    "    one_hot_n = np.zeros(dim)\n",
    "    one_hot_n[n] = 1\n",
    "    return one_hot_n\n",
    "    \n",
    "def undo_one_hot(v):\n",
    "    return np.argmax(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert undo_one_hot(one_hot(5, 10)) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Now let's build our model. We will use an (optional) `Embedding` layer, followed by an `LSTM`. Finally, we will use a `Dense` layer to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(vocab_size, seq_length, embedding=False, embedding_dim=256, lstm_units=1024):\n",
    "    model = Sequential()\n",
    "    if embedding:\n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
    "        model.add(LSTM(units=lstm_units))\n",
    "    else:\n",
    "        model.add(LSTM(units=lstm_units, input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(vocab_size=NUM_CHARS, seq_length=SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1024)              4464640   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 65)                66625     \n",
      "=================================================================\n",
      "Total params: 4,531,265\n",
      "Trainable params: 4,531,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = my_model(vocab_size=NUM_CHARS, seq_length=SEQ_LENGTH, embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1024)              5246976   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 65)                66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models the `None` values for each output shape are for the batch size. The characters from each sequence (of length 5) are embedded in 256-dimensional space (when an embedding layer is used). An LSTM layer expects an input of shape `(timesteps, num_features)`. When an embedding layer is used, the output shape of 5 corresponds to the timesteps, and the 256 is now the number of features (up from the original `NUM_CHARS=65` value). Finally, this flat (ignoring batch size) output (since it only outputs the hidden state from the last timestep) is now passed to the dense layer, which softmax's to get the most probable character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the model *(no embedding layer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1\n",
    "X, y = generate_samples(text, char_to_int, seq_length=SEQ_LENGTH, n_samples=N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(M, vocab_size):\n",
    "    n_samples, seq_length = M.shape\n",
    "    M_oh = np.array([one_hot(r, vocab_size) for r in np.array(M).flatten()]).reshape((n_samples, seq_length, vocab_size))\n",
    "    return np.squeeze(M_oh) # In case this is a target vector, we don't want to include an unnecessary axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh = one_hot_matrix(X, NUM_CHARS)\n",
    "y_oh = one_hot_matrix(y, NUM_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test to see what's happening. We won't train it (just use the initial random weights) and make a prediction on the first training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_prediction(model, X):\n",
    "    example_pred = model.predict(X[0].reshape(1, *X[0].shape), batch_size=1)[0]\n",
    "    return example_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = example_prediction(model, X_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.6236871e-02, 6.3006467e-01, 6.1237547e-03, 9.7289749e-06,\n",
       "       7.6074944e-06, 1.0224881e-02, 4.7651798e-02, 3.8483760e-03,\n",
       "       1.6810777e-02, 7.5542125e-06, 7.7468310e-03, 8.5015539e-03,\n",
       "       5.8913282e-03, 7.6052966e-04, 2.5205009e-04, 2.2003289e-04,\n",
       "       1.4028796e-04, 3.4211713e-04, 1.4300564e-04, 1.8496141e-04,\n",
       "       7.3084095e-04, 7.0533471e-04, 6.0472354e-05, 1.6821121e-04,\n",
       "       1.9451063e-04, 1.9055483e-04, 3.6480208e-04, 2.6066133e-04,\n",
       "       8.7988432e-05, 3.3014010e-05, 2.1228372e-04, 4.4457213e-04,\n",
       "       3.3739515e-04, 2.9324333e-04, 1.3954485e-04, 3.6241428e-04,\n",
       "       9.8123146e-06, 2.1711526e-04, 4.0131850e-05, 1.5786275e-02,\n",
       "       8.6014549e-04, 1.3584991e-03, 1.8380266e-03, 3.8588770e-02,\n",
       "       1.1158638e-03, 4.3081207e-04, 3.2587126e-02, 3.1348344e-02,\n",
       "       6.6493521e-05, 2.2204348e-04, 4.4703628e-03, 1.7170900e-03,\n",
       "       2.9643015e-03, 3.0356895e-02, 1.1907129e-03, 7.0920505e-05,\n",
       "       1.1149459e-02, 6.7667114e-03, 4.0005515e-03, 9.2774872e-03,\n",
       "       1.5996091e-04, 3.4006021e-03, 5.5529981e-05, 2.0110937e-02,\n",
       "       8.4381129e-05], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the suggestion [here](https://www.tensorflow.org/tutorials/text/text_generation) we will *sample* from the softmax distribution (as opposed to just doing `argmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_softmax(output):\n",
    "    sampled_output = np.random.choice(np.arange(len(output)), p=output.reshape(len(output)))\n",
    "    return sampled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [18 47 56 57 58] ('First')\n",
      "Output: v\n"
     ]
    }
   ],
   "source": [
    "sampled_output = sample_from_softmax(example_pred)\n",
    "\n",
    "print(f'Input: {X[0]} (\\'{\"\".join([int_to_char[c] for c in X[0]])}\\')\\nOutput: {int_to_char[sampled_output]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model *(no embedding layer)*\n",
    "We're finally ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10**5\n",
    "X, y = generate_samples(text, char_to_int, seq_length=SEQ_LENGTH, n_samples=N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh = one_hot_matrix(X, NUM_CHARS)\n",
    "y_oh = one_hot_matrix(y, NUM_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 40000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 219s 4ms/sample - loss: 1.9409 - accuracy: 0.4391 - val_loss: 2.0317 - val_accuracy: 0.4114\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 223s 4ms/sample - loss: 1.7833 - accuracy: 0.4744 - val_loss: 1.9816 - val_accuracy: 0.4275\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 223s 4ms/sample - loss: 1.6631 - accuracy: 0.5045 - val_loss: 1.9599 - val_accuracy: 0.4323\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 222s 4ms/sample - loss: 1.5582 - accuracy: 0.5284 - val_loss: 1.9340 - val_accuracy: 0.4467\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 221s 4ms/sample - loss: 1.4618 - accuracy: 0.5510 - val_loss: 1.9274 - val_accuracy: 0.4489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x66818d210>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_oh, y_oh, batch_size=64, epochs=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some predictions and compare to ground truth *(no embedding layer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 63, Actual: 1\n",
      "Predicted: 64, Actual: 64\n",
      "Predicted: 20, Actual: 14\n",
      "Predicted: 43, Actual: 43\n",
      "Predicted: 61, Actual: 54\n",
      "Predicted: 8, Actual: 43\n",
      "Predicted: 42, Actual: 63\n",
      "Predicted: 51, Actual: 58\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 17, Actual: 1\n",
      "Predicted: 43, Actual: 54\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 42, Actual: 10\n",
      "Predicted: 57, Actual: 39\n",
      "Predicted: 59, Actual: 54\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 57, Actual: 57\n",
      "Predicted: 41, Actual: 58\n",
      "Predicted: 1, Actual: 10\n",
      "Predicted: 50, Actual: 1\n",
      "\n",
      "Accuracy: 6/20 = 0.30\n"
     ]
    }
   ],
   "source": [
    "total = 20\n",
    "correct = 0\n",
    "\n",
    "for i in range(total):\n",
    "    pred = sample_from_softmax(example_prediction(model, X_oh[i:]))\n",
    "    actual = undo_one_hot(y_oh[i])\n",
    "    correct += int(pred == actual)\n",
    "    print(f'Predicted: {pred}, Actual: {actual}')\n",
    "    \n",
    "print(f'\\nAccuracy: {correct}/{total} = {correct/total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary:** For the model *without* an embedding layer we trained on 10^5 examples (*not* shuffled) for one epoch and achieved ~30% accuracy (on the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model *(with embedding layer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 40000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 232s 4ms/sample - loss: 1.6653 - accuracy: 0.5038 - val_loss: 1.7489 - val_accuracy: 0.4802\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 233s 4ms/sample - loss: 1.4694 - accuracy: 0.5501 - val_loss: 1.7431 - val_accuracy: 0.4815\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 231s 4ms/sample - loss: 1.3133 - accuracy: 0.5867 - val_loss: 1.7720 - val_accuracy: 0.4823\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 230s 4ms/sample - loss: 1.1745 - accuracy: 0.6233 - val_loss: 1.8296 - val_accuracy: 0.4758\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 230s 4ms/sample - loss: 1.0635 - accuracy: 0.6530 - val_loss: 1.8848 - val_accuracy: 0.4703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x667529b10>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.fit(X, y_oh, batch_size=64, epochs=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0, Actual: 1\n",
      "Predicted: 64, Actual: 64\n",
      "Predicted: 32, Actual: 14\n",
      "Predicted: 43, Actual: 43\n",
      "Predicted: 39, Actual: 54\n",
      "Predicted: 57, Actual: 43\n",
      "Predicted: 42, Actual: 63\n",
      "Predicted: 59, Actual: 58\n",
      "Predicted: 1, Actual: 1\n",
      "Predicted: 58, Actual: 1\n",
      "Predicted: 61, Actual: 54\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 1, Actual: 10\n",
      "Predicted: 57, Actual: 39\n",
      "Predicted: 46, Actual: 54\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 57, Actual: 57\n",
      "Predicted: 58, Actual: 58\n",
      "Predicted: 2, Actual: 10\n",
      "Predicted: 1, Actual: 1\n",
      "\n",
      "Accuracy: 8/20 = 0.40\n"
     ]
    }
   ],
   "source": [
    "total = 20\n",
    "correct = 0\n",
    "\n",
    "for i in range(total):\n",
    "    pred = sample_from_softmax(example_prediction(embed_model, X[i:]))\n",
    "    actual = undo_one_hot(y_oh[i])\n",
    "    correct += int(pred == actual)\n",
    "    print(f'Predicted: {pred}, Actual: {actual}')\n",
    "    \n",
    "print(f'\\nAccuracy: {correct}/{total} = {correct/total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model *with* an embedding layer had a nice increase in accuracy (~20%, or 6 percentage points). Of course, huge grain of salt here since we'e testing on the training set, training for only one epoch, and without a large number of samples. But still, interesting to see.\n",
    "\n",
    "Finally, let's shuffle the samples and train again to see if there is any change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle does not return anything, it shuffles in-place\n",
    "shuffled_idx = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffled_idx)\n",
    "\n",
    "X_oh_shuffled = X_oh[shuffled_idx]\n",
    "X_shuffled = X[shuffled_idx]\n",
    "y_oh_shuffled = y_oh[shuffled_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model *without* embedding layer\n",
    "Note that we are still working with the same (partially trained) model. So this is not a fair comparison in terms of how well shuffling affects the training. To do that we would need to save the model weights *before* training, train, and then reload the model weights before re-training each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 40000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 220s 4ms/sample - loss: 1.5536 - accuracy: 0.5361 - val_loss: 1.5504 - val_accuracy: 0.5333\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 220s 4ms/sample - loss: 1.4275 - accuracy: 0.5599 - val_loss: 1.5622 - val_accuracy: 0.5297\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 226s 4ms/sample - loss: 1.3259 - accuracy: 0.5817 - val_loss: 1.5901 - val_accuracy: 0.5256\n",
      "Epoch 4/5\n",
      " 1024/60000 [..............................] - ETA: 3:18 - loss: 1.1173 - accuracy: 0.6448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-269-8d4b8f808168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_oh_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh_shuffled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/math_lstm/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_oh_shuffled, y_oh_shuffled, batch_size=64, epochs=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model *with* embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.fit(X_shuffled, y_oh_shuffled, batch_size=64, epochs=5, validation_split=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
